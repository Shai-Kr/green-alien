<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Alien Chat for Ari (No Abort Errors!)</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0">
  <style>
    html, body {
      height: 100%;
      margin: 0;
      padding: 0;
      box-sizing: border-box;
    }
    body {
      min-height: 100vh;
      min-width: 100vw;
      background: linear-gradient(to bottom, #e0fff2 70%, #91e3b8 100%);
      display: flex;
      flex-direction: column;
      align-items: center;
      justify-content: center;
      overflow-x: hidden;
      overflow-y: auto;
    }
    .svg-container {
      display: flex;
      justify-content: center;
      align-items: center;
      width: 100vw;
      max-width: 100vw;
      min-height: 60vh;
      box-sizing: border-box;
      background: none;
    }
    svg {
      width: 99vw;
      max-width: 480px;
      min-width: 280px;
      height: auto;
      background: #fff;
      border-radius: 20px;
      border: 4px solid #96deb5;
      box-shadow: 0 2px 10px #b2ddb6b0;
      margin: 0 auto;
      display: block;
    }
    #micBtn {
      margin: 18px 0 0 0;
      padding: 14px 24px;
      background: #6ed39c;
      border: none;
      border-radius: 24px;
      color: #fff;
      font-size: 1.3em;
      cursor: pointer;
      box-shadow: 0 2px 10px #b2ddb6b0;
      transition: background 0.2s;
      width: 90vw;
      max-width: 350px;
      min-width: 160px;
      font-family: inherit;
    }
    #micBtn.active {
      background: #00d100;
    }
    #transcript, #ai-response, #errormsg {
      margin: 8px 0 0 0;
      font-size: 1.08em;
      color: #333;
      min-height: 22px;
      text-align: center;
      word-break: break-word;
      max-width: 95vw;
    }
    #errormsg {
      color: red;
      font-weight: bold;
      min-height: 24px;
    }
    #startOverlay {
      position: fixed;
      left: 0; top: 0; right: 0; bottom: 0;
      background: rgba(80,150,120,0.91);
      z-index: 10;
      color: #fff;
      display: flex;
      flex-direction: column;
      align-items: center;
      justify-content: center;
      font-size: 2em;
      font-family: inherit;
      letter-spacing: 1px;
      transition: opacity 0.7s;
      cursor: pointer;
    }
    #startOverlay h2 {
      margin: 0 0 18px 0;
      font-weight: 600;
      letter-spacing: 2px;
    }
    #startOverlay p {
      font-size: 1em;
      margin: 0;
    }
    @media (max-width: 600px) {
      svg {
        max-width: 99vw;
        min-width: 80vw;
      }
      #micBtn { font-size: 1.02em;}
      #startOverlay { font-size: 1.3em;}
    }
  </style>
</head>
<body>
  <div id="startOverlay">
    <h2>üëΩ Tap to Start!</h2>
    <p>Tap anywhere to activate sound & microphone.<br>
    <small>(Browser blocks audio until you interact)</small></p>
  </div>
  <div class="svg-container">
    <svg id="alienSVG" viewBox="0 0 720 405" xmlns="http://www.w3.org/2000/svg" preserveAspectRatio="xMidYMid meet">
      <image href="alien.jpeg" x="0" y="0" width="720" height="405" />
      <rect id="mouth" x="395" y="105" width="30" height="15" rx="14" fill="#111"/>
    </svg>
  </div>
  <button id="micBtn">üîÅ Listening (auto)</button>
  <div id="transcript"></div>
  <div id="ai-response"></div>
  <div id="errormsg"></div>

<script>
const micBtn = document.getElementById('micBtn');
const transcriptDiv = document.getElementById('transcript');
const aiResponseDiv = document.getElementById('ai-response');
const mouth = document.getElementById('mouth');
const errorDiv = document.getElementById('errormsg');
const startOverlay = document.getElementById('startOverlay');

// **** Put your API KEY here ****
const OPENAI_API_KEY = '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';

let recognition, recognizing = false;
let pendingRecognition = false; // <--- NEW
let autoMode = true;
let lastAudio = null;
let userInteracted = false;
let mouthAnimating = false;
const MAX_TTS_CHARS = 400;

// Context: last 5 user+AI turns (max 10 items)
let conversationHistory = [];

function setMouthClosed() {
  mouth.setAttribute('height', 15);
  mouth.setAttribute('y', 105);
}

function cleanTTS(text) {
  return text
    .replace(/[\u{1F600}-\u{1F6FF}]/gu, '') // emoticons
    .replace(/[*_`~#>\[\]]/g, '') // markdown chars
    .replace(/\n{2,}/g, '\n')
    .replace(/\s{2,}/g, ' ')
    .slice(0, MAX_TTS_CHARS);
}

let lastMicErrorRetry = 0;
let lastTTSErrorRetry = 0;

function animateMouthWhileTalking(audio) {
  mouthAnimating = true;
  let t = 0;
  function loop() {
    if (!mouthAnimating) {
      setMouthClosed();
      return;
    }
    const h = 15 + Math.abs(Math.sin(t/6)) * 30;
    mouth.setAttribute('height', h);
    mouth.setAttribute('y', 105 - (h-15)/2);
    t++;
    if (mouthAnimating) requestAnimationFrame(loop);
  }
  loop();
  audio.onended = () => {
    mouthAnimating = false;
    setMouthClosed();
    if (autoMode && userInteracted) {
      setTimeout(() => { startRecognition(); }, 800);
    }
  };
  audio.onerror = () => {
    errorDiv.textContent = "Audio playback error";
    mouthAnimating = false;
    setMouthClosed();
    if (autoMode && userInteracted) setTimeout(startRecognition, 1200);
  };
}

// --------- SpeechRecognition Setup ---------
if ('webkitSpeechRecognition' in window) {
  recognition = new webkitSpeechRecognition();
  recognition.continuous = false;
  recognition.interimResults = false;
  recognition.lang = 'en-US';

  recognition.onstart = function() {
    recognizing = true;
    pendingRecognition = false; // <-- reset flag
    micBtn.classList.add('active');
    micBtn.innerText = "üîÅ Listening (auto)";
    transcriptDiv.textContent = '';
    errorDiv.textContent = '';
    setMouthClosed();
    mouthAnimating = false;
  };
  recognition.onresult = function(event) {
    const speechResult = event.results[0][0].transcript;
    transcriptDiv.textContent = speechResult;
    recognizing = false;
    pendingRecognition = false;
    micBtn.classList.remove('active');
    micBtn.innerText = "üîÅ Listening (auto)";
    setMouthClosed();
    mouthAnimating = false;

    conversationHistory.push({ role: "user", content: speechResult });
    if (conversationHistory.length > 10) conversationHistory = conversationHistory.slice(-10);

    askOpenAI();
  };
  recognition.onerror = function(event) {
    transcriptDiv.textContent = "Error: " + event.error;
    errorDiv.textContent = "Mic error: " + event.error;
    recognizing = false;
    pendingRecognition = false;
    micBtn.classList.remove('active');
    micBtn.innerText = "üîÅ Listening (auto)";
    setMouthClosed();
    mouthAnimating = false;
    let now = Date.now();
    if (
      (autoMode && userInteracted && now - lastMicErrorRetry > 1800) &&
      event.error !== "not-allowed"
    ) {
      lastMicErrorRetry = now;
      setTimeout(() => { startRecognition(); }, 1800);
    }
  };
  recognition.onend = function() {
    recognizing = false;
    pendingRecognition = false;
    setMouthClosed();
    mouthAnimating = false;
    if (autoMode && userInteracted) setTimeout(() => { startRecognition(); }, 1500);
  };
} else {
  micBtn.disabled = true;
  micBtn.innerText = "Browser not supported üò¢";
  errorDiv.textContent = "This browser does not support speech recognition.";
}

// THE FIX: Use a queue and delay for recognition
function startRecognition() {
  if (!recognition) return;
  if (recognizing || pendingRecognition) return;
  pendingRecognition = true;
  setTimeout(() => {
    try {
      if (!recognizing) recognition.start();
    } catch (e) {
      errorDiv.textContent = "Mic error: " + e.message;
      setTimeout(startRecognition, 1800);
    }
    pendingRecognition = false;
  }, 320); // ◊ì◊ô◊ú◊ô◊ô ◊ß◊ò◊ü (◊ß◊®◊ô◊ò◊ô)
}

micBtn.onclick = function() {
  autoMode = !autoMode;
  errorDiv.textContent = '';
  if (autoMode && userInteracted) {
    micBtn.innerText = "üîÅ Listening (auto)";
    startRecognition();
  } else {
    micBtn.innerText = "üé§ Talk Now";
    recognition && recognition.stop();
    if (lastAudio) {
      lastAudio.pause();
      lastAudio = null;
    }
    setMouthClosed();
    mouthAnimating = false;
  }
};

async function askOpenAI() {
  aiResponseDiv.textContent = "Thinking...";
  errorDiv.textContent = '';
  setMouthClosed();
  mouthAnimating = false;
  try {
    const systemPrompt = `
You are a cheerful, friendly, green alien inspired by the viral Dame Tu Cosita character.
You are talking to a 7-year-old autistic boy named Ari. Your mission is to encourage Ari to speak and communicate.
You prefer to speak English, but you can also use Hebrew sometimes, especially if Ari does. Most of the time, use English, but occasionally use simple Hebrew words or sentences.
Very frequently, throw in funny words or short phrases from the Dame Tu Cosita song (like "cosita", "Dame tu cosita", "ahh", "ay!", "ven aqu√≠", etc).
Always invite Ari to play, repeat words, practice new words and letters, and have fun.
Make every answer sound playful, supportive, and simple. 
Sometimes ask Ari to copy what you say, repeat a word, or play a silly language game.
Never write more than 3 short sentences per message.
`;

    const shortHistory = conversationHistory.slice(-10);

    const openaiResponse = await fetch("https://api.openai.com/v1/chat/completions", {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
        "Authorization": "Bearer " + OPENAI_API_KEY
      },
      body: JSON.stringify({
        model: "gpt-4o",
        messages: [
          { role: "system", content: systemPrompt },
          ...shortHistory
        ]
      })
    }).then(res => res.json());

    const answer = openaiResponse.choices?.[0]?.message?.content || "No response";
    aiResponseDiv.textContent = answer;

    conversationHistory.push({ role: "assistant", content: answer });
    if (conversationHistory.length > 10) conversationHistory = conversationHistory.slice(-10);

    playOpenAITTS(answer, "coral");
  } catch (err) {
    errorDiv.textContent = "OpenAI error: " + err.message;
    setMouthClosed();
    mouthAnimating = false;
    let now = Date.now();
    if (autoMode && userInteracted && now - lastMicErrorRetry > 1800) {
      lastMicErrorRetry = now;
      setTimeout(startRecognition, 1800);
    }
  }
}

async function playOpenAITTS(text, voice="coral") {
  aiResponseDiv.textContent += " (Speaking...)";
  errorDiv.textContent = '';
  setMouthClosed();
  mouthAnimating = false;
  const cleanedText = cleanTTS(text);
  try {
    if (lastAudio) {
      lastAudio.pause();
      lastAudio = null;
    }
    const response = await fetch('https://api.openai.com/v1/audio/speech', {
      method: 'POST',
      headers: {
        "Content-Type": "application/json",
        "Authorization": "Bearer " + OPENAI_API_KEY
      },
      body: JSON.stringify({
        model: "tts-1-hd",    // <- required for "coral"
        input: cleanedText,
        voice: voice,
        response_format: "mp3"
      })
    });

    if (!response.ok) {
      errorDiv.textContent = "TTS Error: " + response.status + " " + response.statusText;
      aiResponseDiv.textContent += " (TTS Error)";
      setMouthClosed();
      mouthAnimating = false;
      let now = Date.now();
      if (autoMode && userInteracted && now - lastTTSErrorRetry > 1200) {
        lastTTSErrorRetry = now;
        setTimeout(() => playOpenAITTS(cleanedText, voice), 1200);
      }
      return;
    }
    const audioData = await response.arrayBuffer();
    const audioBlob = new Blob([audioData], { type: "audio/mp3" });
    const audioUrl = URL.createObjectURL(audioBlob);
    const audio = new Audio(audioUrl);

    if (lastAudio) {
      lastAudio.pause();
      lastAudio = null;
    }
    lastAudio = audio;

    audio.onended = () => {
      mouthAnimating = false;
      setMouthClosed();
      if (autoMode && userInteracted) setTimeout(startRecognition, 800);
    };
    audio.onerror = () => {
      errorDiv.textContent = "Audio playback error";
      mouthAnimating = false;
      setMouthClosed();
      if (autoMode && userInteracted) setTimeout(startRecognition, 1200);
    };
    audio.play().then(() => {
      mouthAnimating = true;
      animateMouthWhileTalking(audio);
    }).catch(err => {
      errorDiv.textContent = "Audio play error: " + err.message;
      setMouthClosed();
      mouthAnimating = false;
      if (autoMode && userInteracted) setTimeout(startRecognition, 1200);
    });

  } catch (err) {
    errorDiv.textContent = "TTS Exception: " + err.message;
    setMouthClosed();
    mouthAnimating = false;
    let now = Date.now();
    if (autoMode && userInteracted && now - lastTTSErrorRetry > 1200) {
      lastTTSErrorRetry = now;
      setTimeout(() => playOpenAITTS(cleanTTS(text), voice), 1200);
    }
  }
}

// --- Overlay: wait for first user tap to allow audio ---
function activateAfterUserTap() {
  userInteracted = true;
  startOverlay.style.opacity = 0;
  setTimeout(() => {
    startOverlay.style.display = "none";
    if (autoMode) setTimeout(startRecognition, 700);
  }, 700);
}
startOverlay.addEventListener("click", activateAfterUserTap);
startOverlay.addEventListener("touchstart", activateAfterUserTap);

setMouthClosed();
</script>
</body>
</html>
