<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Alien Chat for Ari (Stable Auto)</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0">
  <style>
    html, body {
      height: 100%;
      margin: 0;
      padding: 0;
      box-sizing: border-box;
    }
    body {
      min-height: 100vh;
      min-width: 100vw;
      background: linear-gradient(to bottom, #e0fff2 70%, #91e3b8 100%);
      display: flex;
      flex-direction: column;
      align-items: center;
      justify-content: center;
      overflow-x: hidden;
      overflow-y: auto;
    }
    .svg-container {
      display: flex;
      justify-content: center;
      align-items: center;
      width: 100vw;
      max-width: 100vw;
      min-height: 60vh;
      box-sizing: border-box;
      background: none;
    }
    svg {
      width: 99vw;
      max-width: 480px;
      min-width: 280px;
      height: auto;
      background: #fff;
      border-radius: 20px;
      border: 4px solid #96deb5;
      box-shadow: 0 2px 10px #b2ddb6b0;
      margin: 0 auto;
      display: block;
    }
    #micBtn {
      margin: 18px 0 0 0;
      padding: 14px 24px;
      background: #6ed39c;
      border: none;
      border-radius: 24px;
      color: #fff;
      font-size: 1.3em;
      cursor: pointer;
      box-shadow: 0 2px 10px #b2ddb6b0;
      transition: background 0.2s;
      width: 90vw;
      max-width: 350px;
      min-width: 160px;
      font-family: inherit;
    }
    #micBtn.active {
      background: #00d100;
    }
    #transcript, #ai-response, #errormsg {
      margin: 8px 0 0 0;
      font-size: 1.08em;
      color: #333;
      min-height: 22px;
      text-align: center;
      word-break: break-word;
      max-width: 95vw;
    }
    #errormsg {
      color: red;
      font-weight: bold;
      min-height: 24px;
    }
    #startOverlay {
      position: fixed;
      left: 0; top: 0; right: 0; bottom: 0;
      background: rgba(80,150,120,0.91);
      z-index: 10;
      color: #fff;
      display: flex;
      flex-direction: column;
      align-items: center;
      justify-content: center;
      font-size: 2em;
      font-family: inherit;
      letter-spacing: 1px;
      transition: opacity 0.7s;
      cursor: pointer;
    }
    #startOverlay h2 {
      margin: 0 0 18px 0;
      font-weight: 600;
      letter-spacing: 2px;
    }
    #startOverlay p {
      font-size: 1em;
      margin: 0;
    }
    @media (max-width: 600px) {
      svg {
        max-width: 99vw;
        min-width: 80vw;
      }
      #micBtn { font-size: 1.02em;}
      #startOverlay { font-size: 1.3em;}
    }
  </style>
</head>
<body>
  <div id="startOverlay">
    <h2>ğŸ‘½ Tap to Start!</h2>
    <p>Tap anywhere to activate sound & microphone.<br>
    <small>(Browser blocks audio until you interact)</small></p>
  </div>
  <div class="svg-container">
    <svg id="alienSVG" viewBox="0 0 720 405" xmlns="http://www.w3.org/2000/svg" preserveAspectRatio="xMidYMid meet">
      <image href="alien.jpeg" x="0" y="0" width="720" height="405" />
      <rect id="mouth" x="395" y="105" width="30" height="15" rx="14" fill="#111"/>
    </svg>
  </div>
  <button id="micBtn">ğŸ” Listening (auto)</button>
  <div id="transcript"></div>
  <div id="ai-response"></div>
  <div id="errormsg"></div>

<script>
const micBtn = document.getElementById('micBtn');
const transcriptDiv = document.getElementById('transcript');
const aiResponseDiv = document.getElementById('ai-response');
const mouth = document.getElementById('mouth');
const errorDiv = document.getElementById('errormsg');
const startOverlay = document.getElementById('startOverlay');

const OPENAI_API_KEY = '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';

let recognition, recognizing = false;
let autoMode = true;
let lastAudio = null;
let userInteracted = false;
let mouthAnimating = false;

function setMouthClosed() {
  mouth.setAttribute('height', 15);
  mouth.setAttribute('y', 105);
}

function animateMouthWhileTalking(audio) {
  mouthAnimating = true;
  let t = 0;
  function loop() {
    if (!mouthAnimating) {
      setMouthClosed();
      return;
    }
    const h = 15 + Math.abs(Math.sin(t/6)) * 30;
    mouth.setAttribute('height', h);
    mouth.setAttribute('y', 105 - (h-15)/2);
    t++;
    if (mouthAnimating) requestAnimationFrame(loop);
  }
  loop();
  audio.onended = () => {
    mouthAnimating = false;
    setMouthClosed();
    // ×ª×Ÿ delay ×§×¦×¨, ×•××– ×”××©×š ×”××–× ×” (×× ××•×˜×•××˜×™)
    if (autoMode && userInteracted) {
      setTimeout(() => {
        try { if (recognition) recognition.abort(); } catch(e){}
        startRecognition();
      }, 800);
    }
  };
}

if ('webkitSpeechRecognition' in window) {
  recognition = new webkitSpeechRecognition();
  recognition.continuous = false;
  recognition.interimResults = false;
  recognition.lang = 'en-US';

  recognition.onstart = function() {
    recognizing = true;
    micBtn.classList.add('active');
    micBtn.innerText = "ğŸ” Listening (auto)";
    transcriptDiv.textContent = '';
    errorDiv.textContent = '';
    setMouthClosed();
    mouthAnimating = false;
  };
  recognition.onresult = function(event) {
    const speechResult = event.results[0][0].transcript;
    transcriptDiv.textContent = speechResult;
    recognizing = false;
    micBtn.classList.remove('active');
    micBtn.innerText = "ğŸ” Listening (auto)";
    setMouthClosed();
    mouthAnimating = false;
    askOpenAI(speechResult);
  };
  recognition.onerror = function(event) {
    transcriptDiv.textContent = "Error: " + event.error;
    errorDiv.textContent = "Mic error: " + event.error;
    recognizing = false;
    micBtn.classList.remove('active');
    micBtn.innerText = "ğŸ” Listening (auto)";
    setMouthClosed();
    mouthAnimating = false;
    if (autoMode && userInteracted) setTimeout(startRecognition, 1200);
  };
  recognition.onend = function() {
    recognizing = false;
    setMouthClosed();
    mouthAnimating = false;
    if (autoMode && userInteracted) setTimeout(startRecognition, 1200);
  };
} else {
  micBtn.disabled = true;
  micBtn.innerText = "Browser not supported ğŸ˜¢";
  errorDiv.textContent = "This browser does not support speech recognition.";
}

function startRecognition() {
  if (!recognition) return;
  try { recognition.abort(); } catch(e){}
  if (!recognizing) {
    setTimeout(() => {
      setMouthClosed();
      mouthAnimating = false;
      recognition.start();
    }, 150); // ×“×™×œ×™×™ ×§×˜×Ÿ ×œ×× ×™×¢×ª ×”×ª× ×’×©×•×™×•×ª
  }
}

micBtn.onclick = function() {
  autoMode = !autoMode;
  errorDiv.textContent = '';
  if (autoMode && userInteracted) {
    micBtn.innerText = "ğŸ” Listening (auto)";
    startRecognition();
  } else {
    micBtn.innerText = "ğŸ¤ Talk Now";
    recognition && recognition.stop();
    if (lastAudio) {
      lastAudio.pause();
      lastAudio = null;
    }
    setMouthClosed();
    mouthAnimating = false;
  }
};

async function askOpenAI(text) {
  aiResponseDiv.textContent = "Thinking...";
  errorDiv.textContent = '';
  setMouthClosed();
  mouthAnimating = false;
  try {
    const systemPrompt = `You are a cheerful, friendly, green alien, inspired by the viral Dame Tu Cosita character. 
You are talking to a 7-year-old autistic boy named Ari. 
Your mission is to encourage Ari to speak and communicate. 
Be playful, positive, simple, and make every answer sound fun and supportive.`;

    const openaiResponse = await fetch("https://api.openai.com/v1/chat/completions", {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
        "Authorization": "Bearer " + OPENAI_API_KEY
      },
      body: JSON.stringify({
        model: "gpt-3.5-turbo",
        messages: [
          { role: "system", content: systemPrompt },
          { role: "user", content: text }
        ]
      })
    }).then(res => res.json());

    const answer = openaiResponse.choices?.[0]?.message?.content || "No response";
    aiResponseDiv.textContent = answer;
    playOpenAITTS(answer, "echo");
  } catch (err) {
    errorDiv.textContent = "OpenAI error: " + err.message;
    console.error(err);
    setMouthClosed();
    mouthAnimating = false;
    if (autoMode && userInteracted) setTimeout(startRecognition, 1200);
  }
}

async function playOpenAITTS(text, voice="echo") {
  aiResponseDiv.textContent += " (Speaking...)";
  errorDiv.textContent = '';
  setMouthClosed();
  mouthAnimating = false;
  try {
    if (lastAudio) {
      lastAudio.pause();
      lastAudio = null;
    }
    const response = await fetch('https://api.openai.com/v1/audio/speech', {
      method: 'POST',
      headers: {
        "Content-Type": "application/json",
        "Authorization": "Bearer " + OPENAI_API_KEY
      },
      body: JSON.stringify({
        model: "tts-1",
        input: text,
        voice: voice,
        response_format: "mp3"
      })
    });

    if (!response.ok) {
      errorDiv.textContent = "TTS Error: " + response.status + " " + response.statusText;
      aiResponseDiv.textContent += " (TTS Error)";
      setMouthClosed();
      mouthAnimating = false;
      if (autoMode && userInteracted) setTimeout(startRecognition, 1200);
      return;
    }
    const audioData = await response.arrayBuffer();
    const audioBlob = new Blob([audioData], { type: "audio/mp3" });
    const audioUrl = URL.createObjectURL(audioBlob);
    const audio = new Audio(audioUrl);

    if (lastAudio) {
      lastAudio.pause();
      lastAudio = null;
    }
    lastAudio = audio;
    // << ×¨×§ ×›×©×”××•×“×™×• ×‘×××ª ××ª×—×™×œ â€“ ×”×¤×” ×™×–×•×– >>
    audio.onplay = () => { mouthAnimating = true; animateMouthWhileTalking(audio); };
    audio.play().catch(err=>{
      errorDiv.textContent = "Audio play error: " + err.message;
      setMouthClosed();
      mouthAnimating = false;
      if (autoMode && userInteracted) setTimeout(startRecognition, 1200);
    });
  } catch (err) {
    errorDiv.textContent = "TTS Exception: " + err.message;
    setMouthClosed();
    mouthAnimating = false;
    if (autoMode && userInteracted) setTimeout(startRecognition, 1200);
  }
}

// --- Overlay: wait for first user tap to allow audio ---
function activateAfterUserTap() {
  userInteracted = true;
  startOverlay.style.opacity = 0;
  setTimeout(() => {
    startOverlay.style.display = "none";
    if (autoMode) setTimeout(startRecognition, 700);
  }, 700);
}
startOverlay.addEventListener("click", activateAfterUserTap);
startOverlay.addEventListener("touchstart", activateAfterUserTap);

setMouthClosed();
// ×œ× ×œ×”××–×™×Ÿ ××™×“ ×¢×“ ×©×™×© ××™× ×˜×¨××§×¦×™×”!
</script>
</body>
</html>
